Checking for safetensors in venv
safetensors available in venv
Checking for timesfm package in venv
timesfm available in venv
--------- Running TimesFM Benchmark
 See https://github.com/google-research/timesfm/blob/master/README.md for updated APIs.
Loaded PyTorch TimesFM, likely because python version is 3.11.13 (main, Aug 28 2025, 15:10:45) [GCC 8.5.0 20210514 (Red Hat 8.5.0-28)].
Converting safetensors checkpoint /scratch/wd04/sm0074/timesfm/models_pytorch/timesfm-2p5/model.safetensors -> torch .pt temporary file
Converted checkpoint saved to /scratch/wd04/sm0074/timesfm/models_pytorch/timesfm-2p5/model.safetensors.pt
Failed to load 2.5 model offline: Error(s) in loading state_dict for PatchedTimeSeriesDecoder:
	Missing key(s) in state_dict: "input_ff_layer.hidden_layer.0.weight", "input_ff_layer.hidden_layer.0.bias", "input_ff_layer.output_layer.weight", "input_ff_layer.output_layer.bias", "input_ff_layer.residual_layer.weight", "input_ff_layer.residual_layer.bias", "freq_emb.weight", "horizon_ff_layer.hidden_layer.0.weight", "horizon_ff_layer.hidden_layer.0.bias", "horizon_ff_layer.output_layer.weight", "horizon_ff_layer.output_layer.bias", "horizon_ff_layer.residual_layer.weight", "horizon_ff_layer.residual_layer.bias", "stacked_transformer.layers.0.self_attn.scaling", "stacked_transformer.layers.0.self_attn.qkv_proj.weight", "stacked_transformer.layers.0.self_attn.qkv_proj.bias", "stacked_transformer.layers.0.self_attn.o_proj.weight", "stacked_transformer.layers.0.self_attn.o_proj.bias", "stacked_transformer.layers.0.mlp.gate_proj.weight", "stacked_transformer.layers.0.mlp.gate_proj.bias", "stacked_transformer.layers.0.mlp.down_proj.weight", "stacked_transformer.layers.0.mlp.down_proj.bias", "stacked_transformer.layers.0.mlp.layer_norm.weight", "stacked_transformer.layers.0.mlp.layer_norm.bias", "stacked_transformer.layers.0.input_layernorm.weight", "stacked_transformer.layers.1.self_attn.scaling", "stacked_transformer.layers.1.self_attn.qkv_proj.weight", "stacked_transformer.layers.1.self_attn.qkv_proj.bias", "stacked_transformer.layers.1.self_attn.o_proj.weight", "stacked_transformer.layers.1.self_attn.o_proj.bias", "stacked_transformer.layers.1.mlp.gate_proj.weight", "stacked_transformer.layers.1.mlp.gate_proj.bias", "stacked_transformer.layers.1.mlp.down_proj.weight", "stacked_transformer.layers.1.mlp.down_proj.bias", "stacked_transformer.layers.1.mlp.layer_norm.weight", "stacked_transformer.layers.1.mlp.layer_norm.bias", "stacked_transformer.layers.1.input_layernorm.weight", "stacked_transformer.layers.2.self_attn.scaling", "stacked_transformer.layers.2.self_attn.qkv_proj.weight", "stacked_transformer.layers.2.self_attn.qkv_proj.bias", "stacked_transformer.layers.2.self_attn.o_proj.weight", "stacked_transformer.layers.2.self_attn.o_proj.bias", "stacked_transformer.layers.2.mlp.gate_proj.weight", "stacked_transformer.layers.2.mlp.gate_proj.bias", "stacked_transformer.layers.2.mlp.down_proj.weight", "stacked_transformer.layers.2.mlp.down_proj.bias", "stacked_transformer.layers.2.mlp.layer_norm.weight", "stacked_transformer.layers.2.mlp.layer_norm.bias", "stacked_transformer.layers.2.input_layernorm.weight", "stacked_transformer.layers.3.self_attn.scaling", "stacked_transformer.layers.3.self_attn.qkv_proj.weight", "stacked_transformer.layers.3.self_attn.qkv_proj.bias", "stacked_transformer.layers.3.self_attn.o_proj.weight", "stacked_transformer.layers.3.self_attn.o_proj.bias", "stacked_transformer.layers.3.mlp.gate_proj.weight", "stacked_transformer.layers.3.mlp.gate_proj.bias", "stacked_transformer.layers.3.mlp.down_proj.weight", "stacked_transformer.layers.3.mlp.down_proj.bias", "stacked_transformer.layers.3.mlp.layer_norm.weight", "stacked_transformer.layers.3.mlp.layer_norm.bias", "stacked_transformer.layers.3.input_layernorm.weight", "stacked_transformer.layers.4.self_attn.scaling", "stacked_transformer.layers.4.self_attn.qkv_proj.weight", "stacked_transformer.layers.4.self_attn.qkv_proj.bias", "stacked_transformer.layers.4.self_attn.o_proj.weight", "stacked_transformer.layers.4.self_attn.o_proj.bias", "stacked_transformer.layers.4.mlp.gate_proj.weight", "stacked_transformer.layers.4.mlp.gate_proj.bias", "stacked_transformer.layers.4.mlp.down_proj.weight", "stacked_transformer.layers.4.mlp.down_proj.bias", "stacked_transformer.layers.4.mlp.layer_norm.weight", "stacked_transformer.layers.4.mlp.layer_norm.bias", "stacked_transformer.layers.4.input_layernorm.weight", "stacked_transformer.layers.5.self_attn.scaling", "stacked_transformer.layers.5.self_attn.qkv_proj.weight", "stacked_transformer.layers.5.self_attn.qkv_proj.bias", "stacked_transformer.layers.5.self_attn.o_proj.weight", "stacked_transformer.layers.5.self_attn.o_proj.bias", "stacked_transformer.layers.5.mlp.gate_proj.weight", "stacked_transformer.layers.5.mlp.gate_proj.bias", "stacked_transformer.layers.5.mlp.down_proj.weight", "stacked_transformer.layers.5.mlp.down_proj.bias", "stacked_transformer.layers.5.mlp.layer_norm.weight", "stacked_transformer.layers.5.mlp.layer_norm.bias", "stacked_transformer.layers.5.input_layernorm.weight", "stacked_transformer.layers.6.self_attn.scaling", "stacked_transformer.layers.6.self_attn.qkv_proj.weight", "stacked_transformer.layers.6.self_attn.qkv_proj.bias", "stacked_transformer.layers.6.self_attn.o_proj.weight", "stacked_transformer.layers.6.self_attn.o_proj.bias", "stacked_transformer.layers.6.mlp.gate_proj.weight", "stacked_transformer.layers.6.mlp.gate_proj.bias", "stacked_transformer.layers.6.mlp.down_proj.weight", "stacked_transformer.layers.6.mlp.down_proj.bias", "stacked_transformer.layers.6.mlp.layer_norm.weight", "stacked_transformer.layers.6.mlp.layer_norm.bias", "stacked_transformer.layers.6.input_layernorm.weight", "stacked_transformer.layers.7.self_attn.scaling", "stacked_transformer.layers.7.self_attn.qkv_proj.weight", "stacked_transformer.layers.7.self_attn.qkv_proj.bias", "stacked_transformer.layers.7.self_attn.o_proj.weight", "stacked_transformer.layers.7.self_attn.o_proj.bias", "stacked_transformer.layers.7.mlp.gate_proj.weight", "stacked_transformer.layers.7.mlp.gate_proj.bias", "stacked_transformer.layers.7.mlp.down_proj.weight", "stacked_transformer.layers.7.mlp.down_proj.bias", "stacked_transformer.layers.7.mlp.layer_norm.weight", "stacked_transformer.layers.7.mlp.layer_norm.bias", "stacked_transformer.layers.7.input_layernorm.weight", "stacked_transformer.layers.8.self_attn.scaling", "stacked_transformer.layers.8.self_attn.qkv_proj.weight", "stacked_transformer.layers.8.self_attn.qkv_proj.bias", "stacked_transformer.layers.8.self_attn.o_proj.weight", "stacked_transformer.layers.8.self_attn.o_proj.bias", "stacked_transformer.layers.8.mlp.gate_proj.weight", "stacked_transformer.layers.8.mlp.gate_proj.bias", "stacked_transformer.layers.8.mlp.down_proj.weight", "stacked_transformer.layers.8.mlp.down_proj.bias", "stacked_transformer.layers.8.mlp.layer_norm.weight", "stacked_transformer.layers.8.mlp.layer_norm.bias", "stacked_transformer.layers.8.input_layernorm.weight", "stacked_transformer.layers.9.self_attn.scaling", "stacked_transformer.layers.9.self_attn.qkv_proj.weight", "stacked_transformer.layers.9.self_attn.qkv_proj.bias", "stacked_transformer.layers.9.self_attn.o_proj.weight", "stacked_transformer.layers.9.self_attn.o_proj.bias", "stacked_transformer.layers.9.mlp.gate_proj.weight", "stacked_transformer.layers.9.mlp.gate_proj.bias", "stacked_transformer.layers.9.mlp.down_proj.weight", "stacked_transformer.layers.9.mlp.down_proj.bias", "stacked_transformer.layers.9.mlp.layer_norm.weight", "stacked_transformer.layers.9.mlp.layer_norm.bias", "stacked_transformer.layers.9.input_layernorm.weight", "stacked_transformer.layers.10.self_attn.scaling", "stacked_transformer.layers.10.self_attn.qkv_proj.weight", "stacked_transformer.layers.10.self_attn.qkv_proj.bias", "stacked_transformer.layers.10.self_attn.o_proj.weight", "stacked_transformer.layers.10.self_attn.o_proj.bias", "stacked_transformer.layers.10.mlp.gate_proj.weight", "stacked_transformer.layers.10.mlp.gate_proj.bias", "stacked_transformer.layers.10.mlp.down_proj.weight", "stacked_transformer.layers.10.mlp.down_proj.bias", "stacked_transformer.layers.10.mlp.layer_norm.weight", "stacked_transformer.layers.10.mlp.layer_norm.bias", "stacked_transformer.layers.10.input_layernorm.weight", "stacked_transformer.layers.11.self_attn.scaling", "stacked_transformer.layers.11.self_attn.qkv_proj.weight", "stacked_transformer.layers.11.self_attn.qkv_proj.bias", "stacked_transformer.layers.11.self_attn.o_proj.weight", "stacked_transformer.layers.11.self_attn.o_proj.bias", "stacked_transformer.layers.11.mlp.gate_proj.weight", "stacked_transformer.layers.11.mlp.gate_proj.bias", "stacked_transformer.layers.11.mlp.down_proj.weight", "stacked_transformer.layers.11.mlp.down_proj.bias", "stacked_transformer.layers.11.mlp.layer_norm.weight", "stacked_transformer.layers.11.mlp.layer_norm.bias", "stacked_transformer.layers.11.input_layernorm.weight", "stacked_transformer.layers.12.self_attn.scaling", "stacked_transformer.layers.12.self_attn.qkv_proj.weight", "stacked_transformer.layers.12.self_attn.qkv_proj.bias", "stacked_transformer.layers.12.self_attn.o_proj.weight", "stacked_transformer.layers.12.self_attn.o_proj.bias", "stacked_transformer.layers.12.mlp.gate_proj.weight", "stacked_transformer.layers.12.mlp.gate_proj.bias", "stacked_transformer.layers.12.mlp.down_proj.weight", "stacked_transformer.layers.12.mlp.down_proj.bias", "stacked_transformer.layers.12.mlp.layer_norm.weight", "stacked_transformer.layers.12.mlp.layer_norm.bias", "stacked_transformer.layers.12.input_layernorm.weight", "stacked_transformer.layers.13.self_attn.scaling", "stacked_transformer.layers.13.self_attn.qkv_proj.weight", "stacked_transformer.layers.13.self_attn.qkv_proj.bias", "stacked_transformer.layers.13.self_attn.o_proj.weight", "stacked_transformer.layers.13.self_attn.o_proj.bias", "stacked_transformer.layers.13.mlp.gate_proj.weight", "stacked_transformer.layers.13.mlp.gate_proj.bias", "stacked_transformer.layers.13.mlp.down_proj.weight", "stacked_transformer.layers.13.mlp.down_proj.bias", "stacked_transformer.layers.13.mlp.layer_norm.weight", "stacked_transformer.layers.13.mlp.layer_norm.bias", "stacked_transformer.layers.13.input_layernorm.weight", "stacked_transformer.layers.14.self_attn.scaling", "stacked_transformer.layers.14.self_attn.qkv_proj.weight", "stacked_transformer.layers.14.self_attn.qkv_proj.bias", "stacked_transformer.layers.14.self_attn.o_proj.weight", "stacked_transformer.layers.14.self_attn.o_proj.bias", "stacked_transformer.layers.14.mlp.gate_proj.weight", "stacked_transformer.layers.14.mlp.gate_proj.bias", "stacked_transformer.layers.14.mlp.down_proj.weight", "stacked_transformer.layers.14.mlp.down_proj.bias", "stacked_transformer.layers.14.mlp.layer_norm.weight", "stacked_transformer.layers.14.mlp.layer_norm.bias", "stacked_transformer.layers.14.input_layernorm.weight", "stacked_transformer.layers.15.self_attn.scaling", "stacked_transformer.layers.15.self_attn.qkv_proj.weight", "stacked_transformer.layers.15.self_attn.qkv_proj.bias", "stacked_transformer.layers.15.self_attn.o_proj.weight", "stacked_transformer.layers.15.self_attn.o_proj.bias", "stacked_transformer.layers.15.mlp.gate_proj.weight", "stacked_transformer.layers.15.mlp.gate_proj.bias", "stacked_transformer.layers.15.mlp.down_proj.weight", "stacked_transformer.layers.15.mlp.down_proj.bias", "stacked_transformer.layers.15.mlp.layer_norm.weight", "stacked_transformer.layers.15.mlp.layer_norm.bias", "stacked_transformer.layers.15.input_layernorm.weight", "stacked_transformer.layers.16.self_attn.scaling", "stacked_transformer.layers.16.self_attn.qkv_proj.weight", "stacked_transformer.layers.16.self_attn.qkv_proj.bias", "stacked_transformer.layers.16.self_attn.o_proj.weight", "stacked_transformer.layers.16.self_attn.o_proj.bias", "stacked_transformer.layers.16.mlp.gate_proj.weight", "stacked_transformer.layers.16.mlp.gate_proj.bias", "stacked_transformer.layers.16.mlp.down_proj.weight", "stacked_transformer.layers.16.mlp.down_proj.bias", "stacked_transformer.layers.16.mlp.layer_norm.weight", "stacked_transformer.layers.16.mlp.layer_norm.bias", "stacked_transformer.layers.16.input_layernorm.weight", "stacked_transformer.layers.17.self_attn.scaling", "stacked_transformer.layers.17.self_attn.qkv_proj.weight", "stacked_transformer.layers.17.self_attn.qkv_proj.bias", "stacked_transformer.layers.17.self_attn.o_proj.weight", "stacked_transformer.layers.17.self_attn.o_proj.bias", "stacked_transformer.layers.17.mlp.gate_proj.weight", "stacked_transformer.layers.17.mlp.gate_proj.bias", "stacked_transformer.layers.17.mlp.down_proj.weight", "stacked_transformer.layers.17.mlp.down_proj.bias", "stacked_transformer.layers.17.mlp.layer_norm.weight", "stacked_transformer.layers.17.mlp.layer_norm.bias", "stacked_transformer.layers.17.input_layernorm.weight", "stacked_transformer.layers.18.self_attn.scaling", "stacked_transformer.layers.18.self_attn.qkv_proj.weight", "stacked_transformer.layers.18.self_attn.qkv_proj.bias", "stacked_transformer.layers.18.self_attn.o_proj.weight", "stacked_transformer.layers.18.self_attn.o_proj.bias", "stacked_transformer.layers.18.mlp.gate_proj.weight", "stacked_transformer.layers.18.mlp.gate_proj.bias", "stacked_transformer.layers.18.mlp.down_proj.weight", "stacked_transformer.layers.18.mlp.down_proj.bias", "stacked_transformer.layers.18.mlp.layer_norm.weight", "stacked_transformer.layers.18.mlp.layer_norm.bias", "stacked_transformer.layers.18.input_layernorm.weight", "stacked_transformer.layers.19.self_attn.scaling", "stacked_transformer.layers.19.self_attn.qkv_proj.weight", "stacked_transformer.layers.19.self_attn.qkv_proj.bias", "stacked_transformer.layers.19.self_attn.o_proj.weight", "stacked_transformer.layers.19.self_attn.o_proj.bias", "stacked_transformer.layers.19.mlp.gate_proj.weight", "stacked_transformer.layers.19.mlp.gate_proj.bias", "stacked_transformer.layers.19.mlp.down_proj.weight", "stacked_transformer.layers.19.mlp.down_proj.bias", "stacked_transformer.layers.19.mlp.layer_norm.weight", "stacked_transformer.layers.19.mlp.layer_norm.bias", "stacked_transformer.layers.19.input_layernorm.weight". 
	Unexpected key(s) in state_dict: "output_projection_point.hidden_layer.weight", "output_projection_point.output_layer.weight", "output_projection_point.residual_layer.weight", "output_projection_quantiles.hidden_layer.weight", "output_projection_quantiles.output_layer.weight", "output_projection_quantiles.residual_layer.weight", "stacked_xf.0.attn.key_ln.scale", "stacked_xf.0.attn.out.weight", "stacked_xf.0.attn.per_dim_scale.per_dim_scale", "stacked_xf.0.attn.qkv_proj.weight", "stacked_xf.0.attn.query_ln.scale", "stacked_xf.0.ff0.weight", "stacked_xf.0.ff1.weight", "stacked_xf.0.post_attn_ln.scale", "stacked_xf.0.post_ff_ln.scale", "stacked_xf.0.pre_attn_ln.scale", "stacked_xf.0.pre_ff_ln.scale", "stacked_xf.1.attn.key_ln.scale", "stacked_xf.1.attn.out.weight", "stacked_xf.1.attn.per_dim_scale.per_dim_scale", "stacked_xf.1.attn.qkv_proj.weight", "stacked_xf.1.attn.query_ln.scale", "stacked_xf.1.ff0.weight", "stacked_xf.1.ff1.weight", "stacked_xf.1.post_attn_ln.scale", "stacked_xf.1.post_ff_ln.scale", "stacked_xf.1.pre_attn_ln.scale", "stacked_xf.1.pre_ff_ln.scale", "stacked_xf.10.attn.key_ln.scale", "stacked_xf.10.attn.out.weight", "stacked_xf.10.attn.per_dim_scale.per_dim_scale", "stacked_xf.10.attn.qkv_proj.weight", "stacked_xf.10.attn.query_ln.scale", "stacked_xf.10.ff0.weight", "stacked_xf.10.ff1.weight", "stacked_xf.10.post_attn_ln.scale", "stacked_xf.10.post_ff_ln.scale", "stacked_xf.10.pre_attn_ln.scale", "stacked_xf.10.pre_ff_ln.scale", "stacked_xf.11.attn.key_ln.scale", "stacked_xf.11.attn.out.weight", "stacked_xf.11.attn.per_dim_scale.per_dim_scale", "stacked_xf.11.attn.qkv_proj.weight", "stacked_xf.11.attn.query_ln.scale", "stacked_xf.11.ff0.weight", "stacked_xf.11.ff1.weight", "stacked_xf.11.post_attn_ln.scale", "stacked_xf.11.post_ff_ln.scale", "stacked_xf.11.pre_attn_ln.scale", "stacked_xf.11.pre_ff_ln.scale", "stacked_xf.12.attn.key_ln.scale", "stacked_xf.12.attn.out.weight", "stacked_xf.12.attn.per_dim_scale.per_dim_scale", "stacked_xf.12.attn.qkv_proj.weight", "stacked_xf.12.attn.query_ln.scale", "stacked_xf.12.ff0.weight", "stacked_xf.12.ff1.weight", "stacked_xf.12.post_attn_ln.scale", "stacked_xf.12.post_ff_ln.scale", "stacked_xf.12.pre_attn_ln.scale", "stacked_xf.12.pre_ff_ln.scale", "stacked_xf.13.attn.key_ln.scale", "stacked_xf.13.attn.out.weight", "stacked_xf.13.attn.per_dim_scale.per_dim_scale", "stacked_xf.13.attn.qkv_proj.weight", "stacked_xf.13.attn.query_ln.scale", "stacked_xf.13.ff0.weight", "stacked_xf.13.ff1.weight", "stacked_xf.13.post_attn_ln.scale", "stacked_xf.13.post_ff_ln.scale", "stacked_xf.13.pre_attn_ln.scale", "stacked_xf.13.pre_ff_ln.scale", "stacked_xf.14.attn.key_ln.scale", "stacked_xf.14.attn.out.weight", "stacked_xf.14.attn.per_dim_scale.per_dim_scale", "stacked_xf.14.attn.qkv_proj.weight", "stacked_xf.14.attn.query_ln.scale", "stacked_xf.14.ff0.weight", "stacked_xf.14.ff1.weight", "stacked_xf.14.post_attn_ln.scale", "stacked_xf.14.post_ff_ln.scale", "stacked_xf.14.pre_attn_ln.scale", "stacked_xf.14.pre_ff_ln.scale", "stacked_xf.15.attn.key_ln.scale", "stacked_xf.15.attn.out.weight", "stacked_xf.15.attn.per_dim_scale.per_dim_scale", "stacked_xf.15.attn.qkv_proj.weight", "stacked_xf.15.attn.query_ln.scale", "stacked_xf.15.ff0.weight", "stacked_xf.15.ff1.weight", "stacked_xf.15.post_attn_ln.scale", "stacked_xf.15.post_ff_ln.scale", "stacked_xf.15.pre_attn_ln.scale", "stacked_xf.15.pre_ff_ln.scale", "stacked_xf.16.attn.key_ln.scale", "stacked_xf.16.attn.out.weight", "stacked_xf.16.attn.per_dim_scale.per_dim_scale", "stacked_xf.16.attn.qkv_proj.weight", "stacked_xf.16.attn.query_ln.scale", "stacked_xf.16.ff0.weight", "stacked_xf.16.ff1.weight", "stacked_xf.16.post_attn_ln.scale", "stacked_xf.16.post_ff_ln.scale", "stacked_xf.16.pre_attn_ln.scale", "stacked_xf.16.pre_ff_ln.scale", "stacked_xf.17.attn.key_ln.scale", "stacked_xf.17.attn.out.weight", "stacked_xf.17.attn.per_dim_scale.per_dim_scale", "stacked_xf.17.attn.qkv_proj.weight", "stacked_xf.17.attn.query_ln.scale", "stacked_xf.17.ff0.weight", "stacked_xf.17.ff1.weight", "stacked_xf.17.post_attn_ln.scale", "stacked_xf.17.post_ff_ln.scale", "stacked_xf.17.pre_attn_ln.scale", "stacked_xf.17.pre_ff_ln.scale", "stacked_xf.18.attn.key_ln.scale", "stacked_xf.18.attn.out.weight", "stacked_xf.18.attn.per_dim_scale.per_dim_scale", "stacked_xf.18.attn.qkv_proj.weight", "stacked_xf.18.attn.query_ln.scale", "stacked_xf.18.ff0.weight", "stacked_xf.18.ff1.weight", "stacked_xf.18.post_attn_ln.scale", "stacked_xf.18.post_ff_ln.scale", "stacked_xf.18.pre_attn_ln.scale", "stacked_xf.18.pre_ff_ln.scale", "stacked_xf.19.attn.key_ln.scale", "stacked_xf.19.attn.out.weight", "stacked_xf.19.attn.per_dim_scale.per_dim_scale", "stacked_xf.19.attn.qkv_proj.weight", "stacked_xf.19.attn.query_ln.scale", "stacked_xf.19.ff0.weight", "stacked_xf.19.ff1.weight", "stacked_xf.19.post_attn_ln.scale", "stacked_xf.19.post_ff_ln.scale", "stacked_xf.19.pre_attn_ln.scale", "stacked_xf.19.pre_ff_ln.scale", "stacked_xf.2.attn.key_ln.scale", "stacked_xf.2.attn.out.weight", "stacked_xf.2.attn.per_dim_scale.per_dim_scale", "stacked_xf.2.attn.qkv_proj.weight", "stacked_xf.2.attn.query_ln.scale", "stacked_xf.2.ff0.weight", "stacked_xf.2.ff1.weight", "stacked_xf.2.post_attn_ln.scale", "stacked_xf.2.post_ff_ln.scale", "stacked_xf.2.pre_attn_ln.scale", "stacked_xf.2.pre_ff_ln.scale", "stacked_xf.3.attn.key_ln.scale", "stacked_xf.3.attn.out.weight", "stacked_xf.3.attn.per_dim_scale.per_dim_scale", "stacked_xf.3.attn.qkv_proj.weight", "stacked_xf.3.attn.query_ln.scale", "stacked_xf.3.ff0.weight", "stacked_xf.3.ff1.weight", "stacked_xf.3.post_attn_ln.scale", "stacked_xf.3.post_ff_ln.scale", "stacked_xf.3.pre_attn_ln.scale", "stacked_xf.3.pre_ff_ln.scale", "stacked_xf.4.attn.key_ln.scale", "stacked_xf.4.attn.out.weight", "stacked_xf.4.attn.per_dim_scale.per_dim_scale", "stacked_xf.4.attn.qkv_proj.weight", "stacked_xf.4.attn.query_ln.scale", "stacked_xf.4.ff0.weight", "stacked_xf.4.ff1.weight", "stacked_xf.4.post_attn_ln.scale", "stacked_xf.4.post_ff_ln.scale", "stacked_xf.4.pre_attn_ln.scale", "stacked_xf.4.pre_ff_ln.scale", "stacked_xf.5.attn.key_ln.scale", "stacked_xf.5.attn.out.weight", "stacked_xf.5.attn.per_dim_scale.per_dim_scale", "stacked_xf.5.attn.qkv_proj.weight", "stacked_xf.5.attn.query_ln.scale", "stacked_xf.5.ff0.weight", "stacked_xf.5.ff1.weight", "stacked_xf.5.post_attn_ln.scale", "stacked_xf.5.post_ff_ln.scale", "stacked_xf.5.pre_attn_ln.scale", "stacked_xf.5.pre_ff_ln.scale", "stacked_xf.6.attn.key_ln.scale", "stacked_xf.6.attn.out.weight", "stacked_xf.6.attn.per_dim_scale.per_dim_scale", "stacked_xf.6.attn.qkv_proj.weight", "stacked_xf.6.attn.query_ln.scale", "stacked_xf.6.ff0.weight", "stacked_xf.6.ff1.weight", "stacked_xf.6.post_attn_ln.scale", "stacked_xf.6.post_ff_ln.scale", "stacked_xf.6.pre_attn_ln.scale", "stacked_xf.6.pre_ff_ln.scale", "stacked_xf.7.attn.key_ln.scale", "stacked_xf.7.attn.out.weight", "stacked_xf.7.attn.per_dim_scale.per_dim_scale", "stacked_xf.7.attn.qkv_proj.weight", "stacked_xf.7.attn.query_ln.scale", "stacked_xf.7.ff0.weight", "stacked_xf.7.ff1.weight", "stacked_xf.7.post_attn_ln.scale", "stacked_xf.7.post_ff_ln.scale", "stacked_xf.7.pre_attn_ln.scale", "stacked_xf.7.pre_ff_ln.scale", "stacked_xf.8.attn.key_ln.scale", "stacked_xf.8.attn.out.weight", "stacked_xf.8.attn.per_dim_scale.per_dim_scale", "stacked_xf.8.attn.qkv_proj.weight", "stacked_xf.8.attn.query_ln.scale", "stacked_xf.8.ff0.weight", "stacked_xf.8.ff1.weight", "stacked_xf.8.post_attn_ln.scale", "stacked_xf.8.post_ff_ln.scale", "stacked_xf.8.pre_attn_ln.scale", "stacked_xf.8.pre_ff_ln.scale", "stacked_xf.9.attn.key_ln.scale", "stacked_xf.9.attn.out.weight", "stacked_xf.9.attn.per_dim_scale.per_dim_scale", "stacked_xf.9.attn.qkv_proj.weight", "stacked_xf.9.attn.query_ln.scale", "stacked_xf.9.ff0.weight", "stacked_xf.9.ff1.weight", "stacked_xf.9.post_attn_ln.scale", "stacked_xf.9.post_ff_ln.scale", "stacked_xf.9.pre_attn_ln.scale", "stacked_xf.9.pre_ff_ln.scale", "tokenizer.hidden_layer.bias", "tokenizer.hidden_layer.weight", "tokenizer.output_layer.bias", "tokenizer.output_layer.weight", "tokenizer.residual_layer.bias", "tokenizer.residual_layer.weight". 

======================================================================================
                  Resource Usage on 2026-01-04 19:02:39:
   Job Id:             157564468.gadi-pbs
   Project:            wd04
   Exit Status:        0
   Service Units:      0.13
   NCPUs Requested:    12                     NCPUs Used: 12              
                                           CPU Time Used: 00:00:11        
   Memory Requested:   64.0GB                Memory Used: 3.7GB           
   NGPUs Requested:    1                 GPU Utilisation: 0%              
                                         GPU Memory Used: 0B              
   Walltime requested: 10:00:00            Walltime Used: 00:00:13        
   JobFS requested:    1.0GB                  JobFS used: 0B              
======================================================================================
